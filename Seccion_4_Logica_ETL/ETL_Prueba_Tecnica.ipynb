{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba Tecnica - Analista de Procesos ETL\n",
    "",
    "\n",
    "Este notebook contiene las soluciones para las secciones de **Python (Seccion 2)**, las instrucciones de **Excel (Seccion 1)** y la **Logica ETL (Seccion 4)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Seccion 2 - Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Celda 0: Imports\n# ============================================================================\n# pandas: Libreria principal para manipulacion de datos tabulares. Se eligio\n#   porque es el estandar de la industria para ETL en Python y ofrece lectura\n#   nativa de CSV, concatenacion, deduplicacion y exportacion.\n# re: Modulo de expresiones regulares de Python. Se usa para validar formatos\n#   de email con patrones complejos que no se pueden resolver con .contains().\n# warnings: Se desactivan los warnings para mantener la salida limpia.\n#   En produccion se usaria un logger en su lugar.\n# ============================================================================\nimport pandas as pd\nimport re\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Librerias cargadas correctamente.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Carga e Integracion de Archivos\n",
    "\n",
    "Se cargan los dos archivos CSV (separados por `;`) y se unifican en un solo DataFrame.\n",
    "\n",
    "**Nota:** El archivo `flights_5000v2.csv` tiene headers corruptos (Col_6 aparece como Col_7, Col_16 como Col_17, Col_18 como Col_13), por lo que se sobreescriben los nombres de columna al momento de la carga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Celda 1: Seccion 2.1 - Carga e integracion\n# ============================================================================\n# OBJETIVO: Cargar ambos archivos CSV v2 y unificarlos en un solo DataFrame.\n#\n# DECISIONES CLAVE:\n# - sep=';' : Ambos archivos v2 usan punto y coma como separador (no coma).\n#   Esto se verifico inspeccionando las primeras filas de cada archivo.\n#\n# - encoding='utf-8-sig' : Los archivos tienen un BOM (Byte Order Mark) al\n#   inicio (caracter invisible \\ufeff). Sin 'utf-8-sig', el BOM se pega al\n#   nombre de la primera columna, quedando \"\\ufeffCol_1\" en lugar de \"Col_1\".\n#   'utf-8-sig' le indica a pandas que descarte el BOM automaticamente.\n#\n# - names=columnas, header=0 (solo para 5000v2): El archivo flights_5000v2.csv\n#   tiene headers CORRUPTOS: Col_6 aparece como Col_7 (posicion 6), Col_16\n#   como Col_17 (posicion 16), y Col_18 como Col_13 (posicion 18). Esto\n#   causaria que pd.concat() alineara mal las columnas. La solucion es\n#   sobreescribir los headers con los nombres correctos usando 'names' y\n#   descartar la fila original de headers con 'header=0'.\n#   El archivo flights_10000v2.csv tiene headers correctos, por lo que no\n#   necesita esta correccion.\n#\n# - pd.concat() con ignore_index=True: Reinicia el indice del DataFrame\n#   resultante (0, 1, 2, ...) para evitar indices duplicados que podrian\n#   causar problemas en operaciones posteriores.\n# ============================================================================\n\n# Generar lista de headers correctos: ['Col_1', 'Col_2', ..., 'Col_19']\ncolumnas = [f'Col_{i}' for i in range(1, 20)]\n\n# Cargar flights_10000v2.csv - headers correctos, se leen tal cual\ndf_10000 = pd.read_csv('flights_10000v2.csv', sep=';', encoding='utf-8-sig')\nprint(f\"flights_10000v2.csv: {len(df_10000)} registros\")\nprint(f\"  Headers: {list(df_10000.columns)}\")\n\n# Cargar flights_5000v2.csv - headers corruptos, se sobreescriben\ndf_5000 = pd.read_csv('flights_5000v2.csv', sep=';', encoding='utf-8-sig',\n                       names=columnas, header=0)\nprint(f\"\\nflights_5000v2.csv: {len(df_5000)} registros\")\nprint(f\"  Headers (corregidos): {list(df_5000.columns)}\")\n\n# Unificar ambos DataFrames en uno solo\nflights_Union = pd.concat([df_10000, df_5000], ignore_index=True)\nprint(f\"\\n{'='*50}\")\nprint(f\"Total de registros en flights_Union: {len(flights_Union)}\")\nprint(f\"Columnas: {list(flights_Union.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Eliminacion de Duplicados\n",
    "\n",
    "Se eliminan los registros duplicados basandose en la columna `Col_1` (identificador unico), conservando la primera ocurrencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Celda 2: Seccion 2.2 - Eliminacion de duplicados\n# ============================================================================\n# OBJETIVO: Eliminar registros con Col_1 repetido, conservando el primero.\n#\n# DECISIONES CLAVE:\n# - subset=['Col_1'] : Se usa Col_1 como llave de negocio porque es el\n#   identificador unico del vuelo. Los duplicados surgen porque ambos archivos\n#   comparten registros (un pasajero puede aparecer en ambos CSVs con el\n#   mismo Col_1 pero datos diferentes en otras columnas).\n#\n# - keep='first' : Se conserva la PRIMERA ocurrencia. Dado que el concat\n#   puso df_10000 primero, se priorizan los datos del archivo de 10,000.\n#   Esta decision se basa en que el archivo mas grande suele ser la fuente\n#   mas reciente/completa en este contexto.\n#\n# - Se imprime antes/despues para trazabilidad y auditoria del proceso.\n# ============================================================================\n\nprint(f\"Registros ANTES de eliminar duplicados: {len(flights_Union)}\")\n\n# Contar cuantos registros son duplicados (True = es duplicado)\nduplicados = flights_Union.duplicated(subset=['Col_1'], keep='first').sum()\nprint(f\"Registros duplicados encontrados (por Col_1): {duplicados}\")\n\n# Eliminar duplicados conservando la primera ocurrencia\nflights_Sin_Duplicados = flights_Union.drop_duplicates(subset=['Col_1'], keep='first')\n\nprint(f\"Registros DESPUES de eliminar duplicados: {len(flights_Sin_Duplicados)}\")\nprint(f\"\\nRegistros eliminados: {len(flights_Union) - len(flights_Sin_Duplicados)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1 Validacion de Emails - Col_8\n",
    "\n",
    "Se validan los correos electronicos de la columna `Col_8`:\n",
    "- Se eliminan espacios en blanco (padding)\n",
    "- Se aplica una expresion regular para validar el formato de email\n",
    "- Se clasifican en validos e invalidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Celda 3: Seccion 2.3.1 - Validacion de emails Col_8\n# ============================================================================\n# OBJETIVO: Clasificar los emails de Col_8 como validos o invalidos.\n#\n# PROBLEMAS DETECTADOS EN LOS DATOS:\n# - Los emails vienen con ~200 espacios de padding a la derecha\n#   (ej: \"JIM@TERRAPINOVERLAND.COM                                    \")\n# - Hay valores no-email como \"NO TIENE\", \"NA\", \"NO TIEN E\"\n# - Algunos valores tienen formato de email pero les falta el \"@\"\n#\n# DECISIONES CLAVE:\n# - .astype(str) : Convierte a string para evitar errores con valores NaN/None\n#   que pandas podria interpretar como float. Sin esto, .str.strip() fallaria\n#   en filas con valores nulos.\n#\n# - .str.strip() : Elimina los ~200 espacios de padding. Es el primer paso\n#   critico porque sin esto, ningun email pasaria la validacion regex.\n#\n# - Regex r'^[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}$' :\n#   Se uso este patron porque cubre los casos estandar de email:\n#   - Parte local: letras, numeros, puntos, guion bajo, %, +, -\n#   - @ obligatorio: separa la parte local del dominio\n#   - Dominio: letras, numeros, puntos, guiones\n#   - TLD: al menos 2 letras (com, co, org, etc.)\n#   - ^ y $ aseguran que TODO el string sea un email (no solo una parte)\n#   No se uso una libreria como email-validator porque la prueba pide regex.\n#\n# - Se trabaja sobre flights_Sin_Duplicados (no flights_Union) porque la\n#   validacion debe hacerse sobre datos ya deduplicados para evitar contar\n#   el mismo email invalido multiples veces.\n# ============================================================================\n\n# Copiar el DataFrame deduplicado para no modificar el original\ndf = flights_Sin_Duplicados.copy()\n\n# PASO 1: Limpiar espacios en Col_8 (padding de ~200 caracteres)\ndf['Col_8'] = df['Col_8'].astype(str).str.strip()\n\n# PASO 2: Definir regex para validacion de formato email\nemail_regex = r'^[A-Za-z0-9._%+\\-]+@[A-Za-z0-9.\\-]+\\.[A-Za-z]{2,}$'\n\n# PASO 3: Aplicar regex a cada email y clasificar como True/False\ndf['Email_Valido'] = df['Col_8'].apply(lambda x: bool(re.match(email_regex, x)))\n\n# Separar en dos grupos para reporte\nvalidos = df[df['Email_Valido'] == True]\ninvalidos = df[df['Email_Valido'] == False]\n\nprint(f\"Total de registros analizados: {len(df)}\")\nprint(f\"Emails VALIDOS: {len(validos)}\")\nprint(f\"Emails INVALIDOS: {len(invalidos)}\")\n\nprint(f\"\\n--- Ejemplos de emails VALIDOS (primeros 5) ---\")\nprint(validos['Col_8'].head().tolist())\n\nprint(f\"\\n--- Ejemplos de emails INVALIDOS (primeros 10) ---\")\nprint(invalidos['Col_8'].head(10).tolist())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2 Validacion de Telefonos - Col_11\n",
    "\n",
    "Se validan los numeros de telefono de la columna `Col_11`:\n",
    "- Se limpian caracteres especiales (`+`, `-`, espacios)\n",
    "- Reglas de validacion:\n",
    "  - **Celular**: Exactamente 10 digitos, comienza con `3`\n",
    "  - **Fijo**: Exactamente 10 digitos, comienza con `601`\n",
    "- Se clasifican en aptos y no aptos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Celda 4: Seccion 2.3.2 - Validacion de telefonos Col_11\n# ============================================================================\n# OBJETIVO: Clasificar telefonos de Col_11 como aptos o no aptos segun\n# las reglas de numeracion colombiana.\n#\n# PROBLEMAS DETECTADOS EN LOS DATOS:\n# - Formatos mixtos: \"3108152764\", \"1-252-305-6678\", \"+37259197979\"\n# - Algunos tienen guiones, otros tienen prefijo internacional (+)\n# - Longitudes variables (desde 0 digitos hasta 11+)\n# - Valor \"0\" aparece como telefono en algunos registros\n#\n# REGLAS DE VALIDACION (numeracion colombiana):\n# - Celular: Exactamente 10 digitos, inicia con 3 (ej: 3001234567)\n#   Los celulares en Colombia siempre empiezan con 3 y tienen 10 digitos.\n# - Fijo: Exactamente 10 digitos, inicia con 601 (ej: 6011234567)\n#   Desde 2021, los fijos en Colombia usan prefijo de area (60X) + 7 digitos.\n#\n# DECISIONES CLAVE:\n# - Se limpian ANTES de validar: quitar +, -, espacios. Esto normaliza\n#   formatos como \"+57-310-815-2764\" a \"573108152764\" para poder evaluar\n#   la longitud y el prefijo uniformemente.\n# - Se usa .isdigit() como primera validacion para descartar valores\n#   que aun despues de limpiar contengan caracteres no numericos.\n# - Se crea Col_11_Limpio como columna separada para mostrar el resultado\n#   de la limpieza junto al original en los ejemplos de salida.\n# ============================================================================\n\n# PASO 1: Limpiar telefonos - quitar +, -, espacios\n# Se usa regex r'[+\\-\\s]' para eliminar estos caracteres en una sola operacion\ndf['Col_11_Limpio'] = df['Col_11'].astype(str).str.replace(r'[+\\-\\s]', '', regex=True).str.strip()\n\ndef validar_telefono(tel):\n    \"\"\"\n    Valida si un telefono cumple con los formatos colombianos.\n    \n    Retorna:\n    - 'Apto - Celular': 10 digitos, empieza con 3\n    - 'Apto - Fijo': 10 digitos, empieza con 601\n    - 'No Apto': cualquier otro caso (internacional, incompleto, invalido)\n    \"\"\"\n    # Descartar valores no numericos (ej: \"nan\", letras residuales)\n    if not tel.isdigit():\n        return 'No Apto'\n    # Celular colombiano: 10 digitos empezando con 3\n    if len(tel) == 10 and tel.startswith('3'):\n        return 'Apto - Celular'\n    # Fijo colombiano: 10 digitos empezando con 601 (Bogota y area)\n    if len(tel) == 10 and tel.startswith('601'):\n        return 'Apto - Fijo'\n    # Todo lo demas: internacional, longitud incorrecta, prefijo desconocido\n    return 'No Apto'\n\n# PASO 2: Aplicar la funcion de validacion a cada telefono limpio\ndf['Telefono_Estado'] = df['Col_11_Limpio'].apply(validar_telefono)\n\n# PASO 3: Reportar resultados\nprint(\"Clasificacion de telefonos:\")\nprint(df['Telefono_Estado'].value_counts())\n\naptos = df[df['Telefono_Estado'].str.startswith('Apto')]\nno_aptos = df[df['Telefono_Estado'] == 'No Apto']\n\nprint(f\"\\nTotal APTOS: {len(aptos)}\")\nprint(f\"Total NO APTOS: {len(no_aptos)}\")\n\nprint(f\"\\n--- Ejemplos de telefonos APTOS (primeros 5) ---\")\nprint(aptos[['Col_11', 'Col_11_Limpio', 'Telefono_Estado']].head().to_string(index=False))\n\nprint(f\"\\n--- Ejemplos de telefonos NO APTOS (primeros 5) ---\")\nprint(no_aptos[['Col_11', 'Col_11_Limpio', 'Telefono_Estado']].head().to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exportacion del Archivo Limpio\n",
    "\n",
    "Se exporta el DataFrame unificado (`flights_Union`) con las columnas `Col_8` y `Col_11` depuradas como `flights_unificado_limpio.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Celda 5: Seccion 2.4 - Exportacion\n# ============================================================================\n# OBJETIVO: Exportar flights_Union (los 15,000 registros unificados) con\n# las columnas Col_8 y Col_11 ya depuradas como CSV limpio.\n#\n# DECISIONES CLAVE:\n# - Se exporta flights_Union (no flights_Sin_Duplicados): El enunciado\n#   pide exportar los datos unificados con las columnas depuradas. La\n#   deduplicacion fue un ejercicio analitico aparte (seccion 2.2).\n#   Se exportan los 15,000 registros completos con la limpieza aplicada.\n#\n# - .copy() : Se trabaja sobre una copia para no modificar el DataFrame\n#   original flights_Union, que podria necesitarse intacto mas adelante.\n#\n# - Limpieza de Col_8 (strip): Se aplica la misma limpieza que en la\n#   seccion 2.3.1 - quitar los ~200 espacios de padding para que el CSV\n#   exportado sea utilizable sin reprocesar.\n#\n# - Limpieza de Col_11 (quitar +, -, espacios): Se normalizan los telefonos\n#   al formato solo-digitos para consistencia, aplicando la misma limpieza\n#   que en la seccion 2.3.2.\n#\n# - index=False : No incluir el indice de pandas como columna en el CSV,\n#   ya que no aporta informacion de negocio.\n#\n# - encoding='utf-8-sig' : Se usa UTF-8 con BOM para que Excel en Windows\n#   pueda abrir el archivo correctamente con caracteres especiales.\n#   Sin el BOM, Excel podria mostrar caracteres corruptos.\n# ============================================================================\n\n# Copiar flights_Union para aplicar la limpieza sin alterar el original\ndf_exportar = flights_Union.copy()\n\n# Aplicar limpieza de Col_8: quitar espacios de padding (~200 chars)\ndf_exportar['Col_8'] = df_exportar['Col_8'].astype(str).str.strip()\n\n# Aplicar limpieza de Col_11: normalizar telefonos a solo digitos\ndf_exportar['Col_11'] = df_exportar['Col_11'].astype(str).str.replace(r'[+\\-\\s]', '', regex=True).str.strip()\n\n# Exportar a CSV\ndf_exportar.to_csv('flights_unificado_limpio.csv', index=False, encoding='utf-8-sig')\n\nprint(f\"Archivo exportado: flights_unificado_limpio.csv\")\nprint(f\"Total de registros exportados: {len(df_exportar)}\")\nprint(f\"Columnas: {list(df_exportar.columns)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Conexion a Base de Datos\n",
    "\n",
    "Para conectarse a una base de datos **SQL Server** desde Python, se pueden utilizar las librerias `pyodbc` y `sqlalchemy`.\n",
    "\n",
    "### Librerias necesarias\n",
    "```bash\n",
    "pip install pyodbc sqlalchemy\n",
    "```\n",
    "\n",
    "### Parametros de conexion\n",
    "| Parametro | Descripcion | Ejemplo |\n",
    "|-----------|-------------|---------|\n",
    "| DRIVER | Driver ODBC instalado | `{ODBC Driver 17 for SQL Server}` |\n",
    "| SERVER | Nombre o IP del servidor | `localhost` o `192.168.1.100` |\n",
    "| DATABASE | Nombre de la base de datos | `mi_base_datos` |\n",
    "| UID | Usuario de la base de datos | `sa` |\n",
    "| PWD | Contrasena del usuario | `mi_password` |\n",
    "\n",
    "### Ejemplo de conexion con pyodbc\n",
    "```python\n",
    "import pyodbc\n",
    "\n",
    "# Parametros de conexion\n",
    "driver = '{ODBC Driver 17 for SQL Server}'\n",
    "server = 'localhost'\n",
    "database = 'mi_base_datos'\n",
    "uid = 'sa'\n",
    "pwd = 'mi_password'\n",
    "\n",
    "# Cadena de conexion\n",
    "conn_str = f'DRIVER={driver};SERVER={server};DATABASE={database};UID={uid};PWD={pwd}'\n",
    "\n",
    "# Conectar\n",
    "conn = pyodbc.connect(conn_str)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Ejecutar consulta\n",
    "cursor.execute('SELECT TOP 10 * FROM mi_tabla')\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Cerrar conexion\n",
    "conn.close()\n",
    "```\n",
    "\n",
    "### Ejemplo de conexion con SQLAlchemy\n",
    "```python\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Cadena de conexion SQLAlchemy\n",
    "engine = create_engine(\n",
    "    'mssql+pyodbc://sa:mi_password@localhost/mi_base_datos'\n",
    "    '?driver=ODBC+Driver+17+for+SQL+Server'\n",
    ")\n",
    "\n",
    "# Leer datos directamente a un DataFrame\n",
    "df = pd.read_sql('SELECT * FROM mi_tabla', engine)\n",
    "print(df.head())\n",
    "\n",
    "# Escribir un DataFrame a la base de datos\n",
    "df.to_sql('nombre_tabla', engine, if_exists='replace', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Seccion 1 - Excel\n",
    "\n",
    "## Instrucciones paso a paso en Excel con Power Query\n",
    "\n",
    "### Paso 1: Conectar los archivos CSV en Power Query\n",
    "1. Abrir Excel y crear un nuevo libro\n",
    "2. Ir a **Datos > Obtener datos > Desde archivo > Desde texto/CSV**\n",
    "3. Seleccionar `flights_10000v2.csv`\n",
    "4. En la ventana de vista previa:\n",
    "   - Delimitador: **Punto y coma (;)**\n",
    "   - Deteccion de tipo de datos: Primera fila como encabezado\n",
    "5. Hacer clic en **Transformar datos** para abrir Power Query\n",
    "6. Repetir los pasos 2-5 para `flights_5000v2.csv`\n",
    "\n",
    "### Paso 2: Corregir headers del flights_5000v2\n",
    "En Power Query, sobre la consulta de `flights_5000v2`:\n",
    "1. Hacer doble clic en el encabezado de la columna que dice `Col_7` (posicion 6) y renombrar a `Col_6`\n",
    "2. Hacer doble clic en el encabezado de la columna que dice `Col_17` (posicion 16) y renombrar a `Col_16`\n",
    "3. Hacer doble clic en el encabezado de la columna que dice `Col_13` (posicion 18) y renombrar a `Col_18`\n",
    "\n",
    "### Paso 3: Anexar (Append) ambas consultas\n",
    "1. En Power Query, ir a **Inicio > Anexar consultas > Anexar consultas como nuevas**\n",
    "2. Seleccionar la consulta de `flights_10000v2` como tabla principal\n",
    "3. Agregar la consulta de `flights_5000v2` como tabla secundaria\n",
    "4. Hacer clic en **Aceptar**\n",
    "5. Renombrar la nueva consulta como `flights_Union`\n",
    "\n",
    "### Paso 4: Convertir Col_10 de texto a numero\n",
    "1. Seleccionar la columna `Col_10`\n",
    "2. Opcion A: Ir a **Transformar > Tipo de datos** y seleccionar **Numero decimal**\n",
    "3. Opcion B: Clic derecho en el encabezado de `Col_10` > **Cambiar tipo > Numero decimal**\n",
    "4. Si aparecen errores, usar **Reemplazar errores** con valor 0\n",
    "\n",
    "### Paso 5: Cerrar y cargar los datos\n",
    "1. Ir a **Inicio > Cerrar y cargar > Cerrar y cargar en...**\n",
    "2. Seleccionar **Tabla** y elegir la hoja de destino\n",
    "3. Hacer clic en **Aceptar**\n",
    "\n",
    "### Paso 6: Crear tabla dinamica\n",
    "1. Seleccionar cualquier celda dentro de la tabla cargada\n",
    "2. Ir a **Insertar > Tabla dinamica**\n",
    "3. Configurar la tabla dinamica:\n",
    "   - **Filas**: Arrastrar `Col_2` al area de filas\n",
    "   - **Valores**: Arrastrar `Col_1` al area de valores (configurar como **Cuenta**)\n",
    "   - **Valores**: Arrastrar `Col_10` al area de valores (configurar como **Suma**)\n",
    "4. Esto mostrara por cada valor de `Col_2`: la cantidad de registros y la suma de `Col_10`\n",
    "\n",
    "### Paso 7: Identificar el valor mas duplicado de Col_1\n",
    "1. Crear otra tabla dinamica o usar CONTAR.SI\n",
    "2. **Opcion con tabla dinamica**:\n",
    "   - Filas: `Col_1`\n",
    "   - Valores: `Col_1` como **Cuenta**\n",
    "   - Ordenar descendente por cuenta\n",
    "   - El primer valor es el mas duplicado\n",
    "3. **Opcion con formula**:\n",
    "   - En una celda auxiliar: `=INDICE(Col_1;COINCIDIR(MAX(CONTAR.SI(Col_1;Col_1));CONTAR.SI(Col_1;Col_1);0))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Seccion 4 - Logica ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte A: Diseno del Flujo ETL\n",
    "\n",
    "### Escenario\n",
    "Diseno de un flujo ETL para integrar datos desde multiples fuentes (ERP, CRM, archivos externos) hacia un data warehouse centralizado.\n",
    "\n",
    "### 1. Extraccion\n",
    "- **ERP/CRM**: Conexion mediante APIs REST o conectores nativos a la base de datos (JDBC/ODBC). Se usa extraccion **incremental** con columnas de marca de agua (watermark) como `fecha_modificacion` para evitar extraer datos ya procesados.\n",
    "- **Archivos externos**: Recepcion via SFTP o carpetas monitoreadas. Se implementa un patron de deteccion de archivos nuevos con validacion de nombre y estructura antes de procesarlos.\n",
    "- **Frecuencia**: Definida segun requerimientos de negocio (batch diario, horario, o near-real-time segun criticidad).\n",
    "\n",
    "### 2. Validacion\n",
    "- **Schema validation**: Verificar que las columnas esperadas existan, con los tipos de datos correctos.\n",
    "- **Nulos y obligatoriedad**: Verificar campos que no deben ser nulos (PKs, campos criticos de negocio).\n",
    "- **Integridad referencial**: Validar que las llaves foraneas existan en las tablas de referencia.\n",
    "- **Registros rechazados**: Los registros que no pasen validacion se envian a una tabla de rechazo (`reject_table`) con el motivo del error para posterior revision.\n",
    "\n",
    "### 3. Transformacion\n",
    "- **Estandarizacion de formatos**: Normalizar fechas, telefonos, emails, monedas a formatos unificados.\n",
    "- **Deduplicacion**: Identificar y eliminar registros duplicados usando llaves de negocio.\n",
    "- **Logica de negocio**: Aplicar calculos, agregaciones y reglas de negocio especificas (ej: clasificacion de clientes, calculo de metricas).\n",
    "- **Enriquecimiento**: Cruzar datos de distintas fuentes para agregar informacion adicional.\n",
    "\n",
    "### 4. Carga\n",
    "- **Staging area**: Los datos transformados se cargan primero en tablas de staging temporales.\n",
    "- **UPSERT a tablas destino**: Desde staging se ejecuta un UPSERT (INSERT + UPDATE) hacia las tablas definitivas, usando la llave de negocio para determinar si insertar o actualizar.\n",
    "- **Transaccional**: La carga se ejecuta dentro de una transaccion para garantizar atomicidad (todo o nada).\n",
    "\n",
    "### 5. Manejo de Errores\n",
    "- **Logging estructurado**: Cada etapa registra logs con timestamp, etapa, conteo de registros procesados/rechazados, y mensajes de error.\n",
    "- **Alertas**: Notificaciones por email o Slack cuando hay errores criticos o cuando el porcentaje de rechazo supera un umbral.\n",
    "- **Tablas de rechazo**: Registros problematicos se almacenan para reprocesamiento manual o automatico.\n",
    "\n",
    "### 6. Monitoreo\n",
    "- **Dashboard de ejecucion**: Panel con estado de cada ejecucion (exito/fallo), duracion, volumen procesado.\n",
    "- **Conteo de filas por etapa**: Metricas de entrada vs salida en cada fase para detectar perdida o ganancia inesperada de registros.\n",
    "- **SLAs**: Monitoreo de cumplimiento de tiempos de ejecucion esperados.\n",
    "\n",
    "### Diagrama del Flujo\n",
    "```\n",
    "  [ERP/CRM]    [Archivos SFTP]    [APIs externas]\n",
    "      |              |                   |\n",
    "      v              v                   v\n",
    "  +------------------------------------------+\n",
    "  |           EXTRACCION                     |\n",
    "  |   (Incremental / Full / CDC)             |\n",
    "  +------------------------------------------+\n",
    "                    |\n",
    "                    v\n",
    "  +------------------------------------------+\n",
    "  |           VALIDACION                     |\n",
    "  |   Schema + Nulos + Integridad            |\n",
    "  +----+-------------------------------+-----+\n",
    "       |                               |\n",
    "       v                               v\n",
    "  [Registros OK]               [Tabla de Rechazo]\n",
    "       |\n",
    "       v\n",
    "  +------------------------------------------+\n",
    "  |         TRANSFORMACION                   |\n",
    "  |  Estandarizacion + Dedup + Logica Neg.   |\n",
    "  +------------------------------------------+\n",
    "                    |\n",
    "                    v\n",
    "  +------------------------------------------+\n",
    "  |            CARGA                         |\n",
    "  |   Staging -> UPSERT -> Tablas destino    |\n",
    "  +------------------------------------------+\n",
    "                    |\n",
    "                    v\n",
    "  +------------------------------------------+\n",
    "  |         MONITOREO & LOGGING              |\n",
    "  |   Dashboard + Alertas + Metricas         |\n",
    "  +------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte B: Criterios de Calidad de Datos\n",
    "\n",
    "### 1. Controles de Conteo y Checksums\n",
    "- **Conteo por etapa**: Registrar el numero de filas al inicio y final de cada fase (extraccion, validacion, transformacion, carga). Cualquier discrepancia no explicada genera una alerta.\n",
    "- **Checksums**: Para columnas numericas criticas, calcular sumas de verificacion antes y despues de las transformaciones para garantizar que no haya perdida o alteracion de datos.\n",
    "\n",
    "### 2. Validacion con Regex, Rangos e Integridad Referencial\n",
    "- **Regex**: Aplicar expresiones regulares para validar formatos de emails, telefonos, codigos postales, identificaciones.\n",
    "- **Rangos**: Verificar que valores numericos esten dentro de rangos logicos (ej: precios > 0, edades entre 0-150).\n",
    "- **Integridad referencial**: Confirmar que cada FK tenga su correspondiente PK en la tabla maestra.\n",
    "\n",
    "### 3. Evitar Reprocesos\n",
    "- **Operaciones idempotentes**: Disenar el ETL para que si se ejecuta multiples veces con los mismos datos, el resultado sea identico (usar UPSERT en lugar de INSERT).\n",
    "- **Tabla de control**: Mantener una tabla `etl_control` con `batch_id`, `fecha_ejecucion`, `archivo_fuente`, `estado`, `filas_procesadas`. Antes de procesar, verificar si el batch ya fue completado.\n",
    "- **Marcas de agua**: Guardar la ultima fecha/ID procesado para saber desde donde continuar.\n",
    "\n",
    "### 4. Deteccion de Anomalias\n",
    "- **Perfilado estadistico**: Calcular estadisticas basicas (media, desviacion estandar, percentiles) de campos clave. Detectar valores atipicos (outliers) que se desvien significativamente.\n",
    "- **Variacion de volumen**: Alertar si el volumen de datos de una ejecucion varia mas de un porcentaje definido respecto a la ejecucion anterior (ej: +-30%).\n",
    "- **Distribucion de valores**: Monitorear cambios en la distribucion de valores categoricos (ej: si un pais que normalmente tiene 5% de registros de repente tiene 50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte C: Continuidad y Mejora\n",
    "\n",
    "### 1. Manejo de Fallas\n",
    "- **Retry con backoff exponencial**: Ante fallas transitorias (conexion, timeout), reintentar automaticamente con esperas crecientes (1s, 2s, 4s, 8s...) hasta un maximo de intentos.\n",
    "- **Dead Letter Queue (DLQ)**: Registros que fallan repetidamente se envian a una cola/tabla especial para revision manual sin bloquear el flujo principal.\n",
    "- **Rollback transaccional**: Si la carga falla a medio camino, revertir todos los cambios de la transaccion actual para dejar la base de datos en un estado consistente.\n",
    "- **Checkpointing**: Guardar puntos de control intermedios para poder reanudar desde el ultimo punto exitoso en lugar de reprocesar todo.\n",
    "\n",
    "### 2. Documentacion\n",
    "- **Diccionario de datos**: Documentar cada tabla y columna con su descripcion, tipo de dato, reglas de negocio y transformaciones aplicadas.\n",
    "- **Linaje de datos (Data Lineage)**: Rastrear el origen de cada campo en las tablas destino, incluyendo las transformaciones intermedias. Herramientas como Apache Atlas o dbt lineage.\n",
    "- **Runbooks**: Procedimientos operativos documentados para cada escenario de falla (que hacer si falla la extraccion, si hay datos corruptos, si se necesita reprocesar).\n",
    "\n",
    "### 3. Mejoras Futuras\n",
    "- **Migracion a streaming**: Evolucionar de procesamiento batch a near-real-time usando herramientas como Apache Kafka + Spark Streaming para reducir la latencia de datos.\n",
    "- **Catalogo de datos**: Implementar un catalogo centralizado (ej: Apache Atlas, DataHub) para que los usuarios de negocio puedan descubrir y entender los datos disponibles.\n",
    "- **Scoring de calidad**: Asignar un puntaje de calidad a cada registro y tabla basado en completitud, precision, consistencia y frescura. Publicar metricas en un dashboard accesible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}